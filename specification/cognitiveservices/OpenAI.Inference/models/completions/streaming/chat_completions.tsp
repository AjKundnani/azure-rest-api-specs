import "@typespec/rest";
import "@typespec/http";
import "@typespec/versioning";

import "../azure_chat_extensions.tsp";
import "../common.tsp";
import "../functions.tsp";
import "../chat_completions.tsp";

using TypeSpec.Rest;
using TypeSpec.Http;
using TypeSpec.Versioning;

namespace Azure.OpenAI;

@doc("A single, role-attributed message within a chat completion interaction.")
model ChatCompletionsStreamDelta {
  @doc("The role associated with this message payload.")
  @projectedName("json", "role")
  role?: ChatRole;

  #suppress "@azure-tools/typespec-azure-core/no-nullable" "we explicitly want a nullable string"
  @doc("The text associated with this message payload.")
  @projectedName("json", "content")
  content?: string;

  @doc("""
    The name of the author of this message. `name` is required if role is `function`, and it should be the name of the
    function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of
    64 characters.
    """)
  name?: string;

  @doc("The name and arguments of a function that should be called, as generated by the model.")
  @added(ServiceApiVersions.v2023_07_01_Preview)
  @projectedName("json", "function_call")
  functionCall?: FunctionCallStreamDelta;

  @doc("""
    Additional context data associated with a chat message when requesting chat completions using compatible Azure
    OpenAI chat extensions. This includes information like the intermediate data source retrievals used to service a
    request.
    This context information is only populated when using Azure OpenAI with chat extensions capabilities configured.
  """)
  @added(ServiceApiVersions.v2023_08_01_Preview)
  @projectedName("csharp", "AzureExtensionsContext")
  context?: AzureChatExtensionsMessageContext;
}

@doc("""
The representation of a single prompt completion as part of an overall chat completions request.
Generally, `n` choices are generated per provided prompt with a default value of 1.
Token limits and other settings may limit the number of choices generated.
""")
model ChatChoiceStreamChunk {
  @doc("The ordered index associated with this chat completions choice.")
  @projectedName("json", "index")
  index: int32;

  #suppress "@azure-tools/typespec-azure-core/no-nullable" "The operation already returns nulls"
  #suppress "@azure-tools/typespec-autorest/union-unsupported" "OpenAPI v2 support deferred"
  @doc("The reason that this chat completions choice completed its generated.")
  @projectedName("json", "finish_reason")
  finishReason: CompletionsFinishReason | null;

  @doc("The delta message content for a streaming response.")
  @projectedName("json", "delta")
  @projectedName("csharp", "InternalStreamingDeltaMessage")
  delta: ChatCompletionsStreamDelta;

  @doc("""
    Information about the content filtering category (hate, sexual, violence, self_harm), if it
    has been detected, as well as the severity level (very_low, low, medium, high-scale that
    determines the intensity and risk level of harmful content) and if it has been filtered or not.
    """)
  @added(ServiceApiVersions.v2023_06_01_Preview)
  @projectedName("json", "content_filter_results")
  contentFilterResults?: ContentFilterResults;
}

@doc("""
Representation of the response data from a chat completions request.
Completions support a wide variety of tasks and generate text that continues from or "completes"
provided prompt data.
""")
model ChatCompletionsStream {
  @doc("A unique identifier associated with this chat completions response.")
  @projectedName("json", "id")
  id: string;

  @doc("""
    The first timestamp associated with generation activity for this completions response,
    represented as seconds since the beginning of the Unix epoch of 00:00 on 1 Jan 1970.
    """)
  @projectedName("json", "created")
  @projectedName("java", "createdAt")
  @encode(DateTimeKnownEncoding.unixTimestamp, int32)
  created: utcDateTime;

  @doc("""
    The collection of completions choices associated with this completions response.
    Generally, `n` choices are generated per provided prompt with a default value of 1.
    Token limits and other settings may limit the number of choices generated.
    """)
  @projectedName("json", "choices")
  choices: ChatChoiceStreamChunk[];

  @doc("""
  Content filtering results for zero or more prompts in the request. In a streaming request,
  results for different prompts may arrive at different times or in different orders.
  """)
  @added(ServiceApiVersions.v2023_06_01_Preview)
  @projectedName("json", "prompt_filter_results")
  promptFilterResults?: PromptFilterResult[];

  @doc("""
    Usage information for tokens processed and generated as part of this completions operation.
    """)
  @projectedName("json", "usage")
  usage: CompletionsUsage;
}
